{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T14:31:50.136312900Z",
     "start_time": "2023-09-20T14:31:45.497856500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# from ckiptagger import data_utils, WS\n",
    "# ws = WS(\"./data\",disable_cuda=False) # ckiptagger 模型 \n",
    "\n",
    "import transformers\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "# from transformers import BertTokenizerFast, AutoModel\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('ckiplab/albert-base-chinese-ner')\n",
    "# model = AutoModel.from_pretrained('ckiplab/albert-base-chinese-ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f15447c3097f0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T14:32:00.603788900Z",
     "start_time": "2023-09-20T14:31:50.125831500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 載入模型，device參數 0 = GPU, -1 = CPU\n",
    "ckip_ws = CkipWordSegmenter(model=\"bert-base\",device=-1)\n",
    "ckip_pos = CkipPosTagger(model=\"bert-base\",device=-1)\n",
    "ckip_ner = CkipNerChunker(model='bert-base',device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac229c0057be29cc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T14:32:00.603788900Z",
     "start_time": "2023-09-20T14:32:00.510566400Z"
    }
   },
   "outputs": [],
   "source": [
    "# text = ['這價位真的很不行，全部只有甜點上得了檯面，特別是廁所，難以置信五星級飯店的廁所連免治馬桶都沒有（可能在房間裡面），整間灰灰暗暗，牆壁磁磚顏色還以為是發霉，真的很對不起一起來聚餐的同事，完全不推薦。']\n",
    "# \n",
    "# pos_text = ckip_ws(text)\n",
    "# print(pos_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b95d773f945ada",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 原始評論留言文字檔處理\n",
    "### 目標\n",
    "    - 依照分類爬蟲獲取每家餐廳評論文字檔轉換為excel逐行儲存每家餐廳評論\n",
    "        - excel檔案欄位表\n",
    "            - 評論留言原內容\n",
    "            - 評論留言基本處理 (去除表情符號、無意義字符)\n",
    "            - 斷詞處理內容\n",
    "            - 停用詞處理內容\n",
    "\n",
    "### 調整紀錄\n",
    "    - 留言中有些消費者的文章中有用到,所以在後續將每則評論留言以,作為分隔識別一則一則存入一行一行dataframe中會造成消費者文章中使用的,將本來是同一篇評論分隔開成好幾則造成資料錯誤傳入\n",
    "        - 本來是將評論.txt做完一些基礎處裡後在輸出為新的評論.txt到新的檔案路徑中，現在改成直接把未處理的評論.txt直接先依照列表形式逐筆存入dataframe中\n",
    "    - 不少評論留言中出現 👍表情符號，這個表情符號應該有高度正向意思，不過會在資料處理中與其他表情符號被去除，所以在comment_text_basic_processing函式中預先替換為 '讚'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504af6cc420fdcd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 檔案讀寫path\n",
    "raw_restaurant_comments_folder = r\"\\專案實作\\地區餐廳評論分析\\google_restaurant_comments_txt\\restaurant_comments_unprocessed\"\n",
    "revised_restaurant_comments_folder = r\"\\專案實作\\地區餐廳評論分析\\google_restaurant_comments_txt\\restaurant_comments_processed\"\n",
    "\n",
    "\n",
    "def comment_text_basic_processing(data,col):\n",
    "    \"\"\"\n",
    "    留言中表情符號字串去除\n",
    "    特定表情符號轉換為對應意思中文詞彙\n",
    "    :param data: \n",
    "    :param col: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    data[col] = data[col].str.replace('👍','讚')\n",
    "    \n",
    "    emoji_text = re.compile(\n",
    "        r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F700-\\U0001F77F'\n",
    "        r'\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF'\n",
    "        r'\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF'\n",
    "        r'\\U0001FC00-\\U0001FCFF\\U0001FD00-\\U0001FDFF\\U0001FE00-\\U0001FEFF'\n",
    "        r'\\U0001FF00-\\U0001FFFF\\u2B05\\u2B06\\u2B07\\u2934\\u2935\\u25AA\\u25FE'\n",
    "        r'\\u2B05\\u2B06\\u2B07\\u2934\\u2935\\u25AA\\u25FE\\u2600-\\u26FF\\u2700-\\u27BF'\n",
    "        r'\\u2300-\\u23FF\\ufe0f]+',\n",
    "        flags=re.UNICODE)\n",
    "    data['評論留言原內容處理'] = data[col].apply(lambda x:emoji_text.sub('',x))\n",
    "\n",
    "\n",
    "# ckiptagger 中文斷詞處理\n",
    "# def segment_process(text):\n",
    "#     \"\"\"\n",
    "#     評論留言斷詞\n",
    "#     斷詞以 | 分隔\n",
    "#     :param text: \n",
    "#     :return: \n",
    "#     \"\"\"\n",
    "#     content_text = ws([text])\n",
    "#     return '|'.join(content_text[0])\n",
    "\n",
    "\n",
    "# ckip Transformers nlp bert model 斷詞\n",
    "def segment_process_2(text):\n",
    "    \"\"\"\n",
    "    :param text: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    content_text = ckip_ws([text])\n",
    "    return '|'.join(content_text[0])\n",
    "\n",
    "\n",
    "def pos_process(text):\n",
    "    content_text = ckip_pos([text])\n",
    "    return  list(zip(text,content_text))\n",
    "\n",
    "\n",
    "# def ner_process(text):\n",
    "#     content_text = ckip_ner([text])\n",
    "#     return  content_text\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(raw_restaurant_comments_folder):\n",
    "    for dir in dirs:\n",
    "        print(\"要處理的餐廳類別:\",dir)\n",
    "    for file_name in files:\n",
    "        print(\"要處理的評論文字檔:\",os.path.join(root,file_name))\n",
    "    \n",
    "        if file_name.endswith(\".txt\"):\n",
    "            txt_file_path = os.path.join(root, file_name)\n",
    "            file_name = os.path.basename(txt_file_path)\n",
    "            # print(f\"正在處理{file_name}評論文字檔...\")\n",
    "        \n",
    "            # 創建對應同名餐廳類別資料夾\n",
    "            sub_folder_name = os.path.basename(root)\n",
    "            revised_sub_folder_path = os.path.join(revised_restaurant_comments_folder,sub_folder_name)\n",
    "            os.makedirs(revised_sub_folder_path,exist_ok=True)\n",
    "            # print(f\"已創建{revised_sub_folder_path}資料夾\")\n",
    "\n",
    "            # 打開原始評論文字檔案轉換為dataframe\n",
    "            try:\n",
    "                with open(txt_file_path, 'r',encoding='utf-8') as file:\n",
    "                    file_content = file.read()\n",
    "                comments_str_list = file_content.split(',') # data type is list\n",
    "                df_comments = pd.DataFrame(data=comments_str_list,columns=['評論留言原內容'])\n",
    "                \n",
    "                # 對評論留言原內容做基本處理存放於新欄位\n",
    "                comment_text_basic_processing(df_comments,'評論留言原內容')\n",
    "                df_comments['評論留言原內容處理'] = df_comments['評論留言原內容處理'].str.replace('[', '').str.replace(']', '').str.replace('\\\\n', '').str.replace(\"'\", '').str.replace('\\\\u200d','')\n",
    "                \n",
    "                # 對評論留言原內容處理後進行斷詞處理\n",
    "                df_comments['評論留言內容斷詞'] = df_comments['評論留言原內容處理'].apply(segment_process_2)\n",
    "                \n",
    "                # 對評論留言斷詞內容進行詞性標註\n",
    "                # df_comments['評論留言詞性'] = df_comments['評論留言內容斷詞'].apply(pos_process)\n",
    "                \n",
    "                # 轉換為dataframe後輸出為excel檔案\n",
    "                df_comments_file_name = os.path.splitext(file_name)[0] + '.xlsx'\n",
    "                df_comments_file_path = os.path.join(revised_sub_folder_path,df_comments_file_name)\n",
    "                df_comments.to_excel(df_comments_file_path,index=False)\n",
    "                print(f\"已將{txt_file_path} --> excel檔案，且轉存至{revised_sub_folder_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"轉換文件{txt_file_path}，出現錯誤:{str(e)}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 進行斷詞處理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f2eec3d051d6ac0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c2598b5badacea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# \n",
    "# # 設定主資料夾路徑\n",
    "# main_folder = r'\\專案實作\\地區餐廳評論分析\\google_restaurant_comments_txt\\restaurant_comments_processed'\n",
    "# \n",
    "# # 初始化變量用於存儲每個子資料夾的資料行數和Excel文件數量\n",
    "# total_rows = 0\n",
    "# total_files = 0\n",
    "# \n",
    "# # 遍歷主資料夾中的所有子資料夾\n",
    "# for folder in os.listdir(main_folder):\n",
    "#     folder_path = os.path.join(main_folder, folder)\n",
    "#     \n",
    "#     # 檢查是否為資料夾\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # 在子資料夾中查找所有Excel文件\n",
    "#         excel_files = glob.glob(os.path.join(folder_path, '*.xlsx')) + glob.glob(os.path.join(folder_path, '*.xls'))\n",
    "#         \n",
    "#         # 初始化變量用於存儲當前子資料夾的資料行數和Excel文件數量\n",
    "#         folder_rows = 0\n",
    "#         folder_files = len(excel_files)\n",
    "#         \n",
    "#         # 遍歷所有Excel文件\n",
    "#         for excel_file in excel_files:\n",
    "#             # 讀取Excel文件\n",
    "#             df = pd.read_excel(excel_file)\n",
    "#             \n",
    "#             # 獲取資料行數\n",
    "#             num_rows = df.shape[0]\n",
    "#             \n",
    "#             # 打印每個檔案的資料行數\n",
    "#             print(f\"檔案：{excel_file}，資料行數：{num_rows}\")\n",
    "#             \n",
    "#             # 更新當前子資料夾的資料行數\n",
    "#             folder_rows += num_rows\n",
    "#         \n",
    "#         # 更新總資料行數和總Excel文件數量\n",
    "#         total_rows += folder_rows\n",
    "#         total_files += folder_files\n",
    "#         \n",
    "#         # 計算當前子資料夾的平均資料行數\n",
    "#         if folder_files > 0:\n",
    "#             average_rows = folder_rows / folder_files\n",
    "#             print(f\"子資料夾：{folder}，平均資料行數：{average_rows}\")\n",
    "#         else:\n",
    "#             print(f\"子資料夾：{folder}，沒有Excel文件。\")\n",
    "# \n",
    "# # 計算所有子資料夾的平均資料行數\n",
    "# if total_files > 0:\n",
    "#     overall_average = total_rows / total_files\n",
    "#     print(f\"所有子資料夾的平均資料行數：{overall_average}\")\n",
    "# else:\n",
    "#     print(\"沒有Excel文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from ckiptagger import data_utils, construct_dictionary, WS, POS\n",
    "# \n",
    "# # 載入模型\n",
    "# ws = WS(\"./data\")\n",
    "# pos = POS(\"./data\")\n",
    "# \n",
    "# # 設定停用詞\n",
    "# stop_words = ['的', '了', '在', '是', '有', '我', '他', '她', '你', '妳']\n",
    "# \n",
    "# # 處理文本\n",
    "# text = \"這是一段中文文本，我們要對它進行停用詞處理。\"\n",
    "# word_sentence_list = ckip_ws([text])\n",
    "# pos_sentence_list = pos(word_sentence_list)\n",
    "# \n",
    "# # 去除停用詞\n",
    "# for i, sentence in enumerate(word_sentence_list):\n",
    "#     for j, word in enumerate(sentence):\n",
    "#         if word in stop_words:\n",
    "#             word_sentence_list[i][j] = ''\n",
    "# print(word_sentence_list)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4b2c94b74bb8646"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
